
CondaError: Run 'conda init' before 'conda activate'

/home/gganeshl/miniconda3/envs/speech/lib/python3.9/site-packages/espnet2/enh/loss/criterions/time_domain.py:446: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/gganeshl/miniconda3/envs/speech/lib/python3.9/site-packages/espnet2/enh/encoder/stft_encoder.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]Fetching 8 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 5432.16it/s]
/home/gganeshl/miniconda3/envs/speech/bin/python /home/gganeshl/Aura/accent_adaptive_asr/asr_ft.py
/home/gganeshl/miniconda3/envs/speech/bin/python /home/gganeshl/Aura/accent_adaptive_asr/asr_ft.py
wandb: Currently logged in as: gganeshl (gganeshl-carnegie-mellon-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /data/user_data/gganeshl/owsm_v3.2/exp/finetune/wandb/run-20250422_185622-6w6mxly7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-thunder-3
wandb: â­ï¸ View project at https://wandb.ai/gganeshl-carnegie-mellon-university/ESPnet_ESPnetEZDataTask
wandb: ðŸš€ View run at https://wandb.ai/gganeshl-carnegie-mellon-university/ESPnet_ESPnetEZDataTask/runs/6w6mxly7
wandb: WARNING Serializing object of type list that is 400088 bytes
wandb: WARNING Serializing object of type list that is 444376 bytes
/home/gganeshl/miniconda3/envs/speech/lib/python3.9/site-packages/espnet2/train/trainer.py:638: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(
/home/gganeshl/miniconda3/envs/speech/lib/python3.9/site-packages/espnet2/s2t/espnet_model.py:279: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(False):
/home/gganeshl/miniconda3/envs/speech/lib/python3.9/site-packages/espnet2/train/trainer.py:856: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(
/home/gganeshl/miniconda3/envs/speech/lib/python3.9/site-packages/espnet2/train/trainer.py:638: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(
/home/gganeshl/miniconda3/envs/speech/lib/python3.9/site-packages/espnet2/s2t/espnet_model.py:279: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(False):
/home/gganeshl/miniconda3/envs/speech/lib/python3.9/site-packages/espnet2/train/trainer.py:856: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                   epoch â–â–ƒâ–†â–ˆ
wandb:                               iteration â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                             metrics/acc â–â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                   metrics/backward_time â–†â–…â–…â–„â–…â–…â–„â–…â–…â–…â–â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                            metrics/clip â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    metrics/forward_time â–ˆâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                       metrics/grad_norm â–ˆâ–‡â–‡â–‡â–†â–†â–ˆâ–ˆâ–‡â–‡â–†â–‡â–†â–†â–‡â–†â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚
wandb:                       metrics/iter_time â–†â–…â–„â–„â–„â–ƒâ–„â–„â–„â–„â–„â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ˆâ–„â–„â–…â–…â–…â–„â–„â–„â–„â–…â–…â–…â–„â–…â–…â–…â–…â–…â–…
wandb:                            metrics/loss â–ˆâ–‡â–†â–†â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        metrics/loss_att â–ˆâ–‡â–‡â–†â–†â–…â–…â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        metrics/loss_ctc â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–…â–„â–„â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                      metrics/loss_scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                      metrics/optim0_lr0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 metrics/optim_step_time â–ˆâ–…â–„â–„â–„â–„â–„â–„â–„â–„â–‚â–â–â–â–â–â–â–‚â–‚â–‚â–„â–„â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚
wandb:                   train/train_acc_epoch â–â–†â–‡â–ˆ
wandb:         train/train_backward_time_epoch â–„â–â–„â–ˆ
wandb:                  train/train_clip_epoch â–â–â–â–
wandb:          train/train_forward_time_epoch â–ˆâ–â–„â–‚
wandb: train/train_gpu_max_cached_mem_GB_epoch â–â–ˆâ–ˆâ–ˆ
wandb:             train/train_grad_norm_epoch â–ˆâ–ƒâ–‚â–
wandb:             train/train_iter_time_epoch â–„â–â–…â–ˆ
wandb:              train/train_loss_att_epoch â–ˆâ–ƒâ–‚â–
wandb:              train/train_loss_ctc_epoch â–ˆâ–„â–‚â–
wandb:                  train/train_loss_epoch â–ˆâ–ƒâ–‚â–
wandb:            train/train_loss_scale_epoch â–â–â–â–
wandb:            train/train_optim0_lr0_epoch â–â–â–â–
wandb:       train/train_optim_step_time_epoch â–ˆâ–â–†â–‚
wandb:                        train/train_time â–…â–…â–„â–„â–„â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…
wandb:            train/train_train_time_epoch â–„â–â–…â–ˆ
wandb:                   valid/valid_acc_epoch â–ˆâ–…â–‚â–
wandb:               valid/valid_cer_ctc_epoch â–â–ˆâ–†â–†
wandb:                   valid/valid_cer_epoch â–â–„â–‡â–ˆ
wandb: valid/valid_gpu_max_cached_mem_GB_epoch â–â–ˆâ–ˆâ–ˆ
wandb:              valid/valid_loss_att_epoch â–â–„â–†â–ˆ
wandb:              valid/valid_loss_ctc_epoch â–â–„â–†â–ˆ
wandb:                  valid/valid_loss_epoch â–â–„â–†â–ˆ
wandb:                   valid/valid_wer_epoch â–â–„â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:                                   epoch 4
wandb:                               iteration 2000
wandb:                             metrics/acc 0.97137
wandb:                   metrics/backward_time 0.09518
wandb:                            metrics/clip 100
wandb:                    metrics/forward_time 0.10156
wandb:                       metrics/grad_norm 37.38352
wandb:                       metrics/iter_time 1.9105
wandb:                            metrics/loss 1.07486
wandb:                        metrics/loss_att 3.03668
wandb:                        metrics/loss_ctc 7.24582
wandb:                      metrics/loss_scale 1
wandb:                      metrics/optim0_lr0 0.0001
wandb:                 metrics/optim_step_time 0.00973
wandb:                   train/train_acc_epoch 0.97137
wandb:         train/train_backward_time_epoch 0.09518
wandb:                  train/train_clip_epoch 100
wandb:          train/train_forward_time_epoch 0.10156
wandb: train/train_gpu_max_cached_mem_GB_epoch 28.85742
wandb:             train/train_grad_norm_epoch 37.38352
wandb:             train/train_iter_time_epoch 1.9105
wandb:              train/train_loss_att_epoch 3.03668
wandb:              train/train_loss_ctc_epoch 7.24582
wandb:                  train/train_loss_epoch 1.07486
wandb:            train/train_loss_scale_epoch 1
wandb:            train/train_optim0_lr0_epoch 0.0001
wandb:       train/train_optim_step_time_epoch 0.00973
wandb:                        train/train_time 8.54279
wandb:            train/train_train_time_epoch 8.54279
wandb:                   valid/valid_acc_epoch 0.87014
wandb:               valid/valid_cer_ctc_epoch 0.09164
wandb:                   valid/valid_cer_epoch 0.08518
wandb: valid/valid_gpu_max_cached_mem_GB_epoch 28.85742
wandb:              valid/valid_loss_att_epoch 14.22787
wandb:              valid/valid_loss_ctc_epoch 16.60744
wandb:                  valid/valid_loss_epoch 14.94174
wandb:                   valid/valid_wer_epoch 0.91
wandb: 
wandb: ðŸš€ View run classic-thunder-3 at: https://wandb.ai/gganeshl-carnegie-mellon-university/ESPnet_ESPnetEZDataTask/runs/6w6mxly7
wandb: â­ï¸ View project at: https://wandb.ai/gganeshl-carnegie-mellon-university/ESPnet_ESPnetEZDataTask
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /data/user_data/gganeshl/owsm_v3.2/exp/finetune/wandb/run-20250422_185622-6w6mxly7/logs
